<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>分布式调度架构</title>
      <link href="/%E5%88%86%E5%B8%83%E5%BC%8F%E8%B0%83%E5%BA%A6%E6%9E%B6%E6%9E%84/"/>
      <url>/%E5%88%86%E5%B8%83%E5%BC%8F%E8%B0%83%E5%BA%A6%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<p>分布式系统为用户任务寻找合适的服务器的过程叫做<strong>调度</strong>。分布式架构中<strong>调度器</strong>是一个非常重要的部分，调度器提供多种调度策略并完成具体的调度工作。常见的调度原理有单体调度、两层调度、共享状态调度。</p><h1>单体调度</h1><p>集群中只有一个节点运行调度进程，该节点对集群中的其他节点具有访问权限，可以搜集其他节点的状态、资源信息等进行统一管理，同时结合用户任务对资源的需求，在调度器中把任务与资源进行匹配并根据匹配结果将任务指派给合适节点的调度方式叫做单体调度。</p><p><strong>单体调度器拥有全局资源视图和全局任务，可以很容易地实现对任务的约束并实施全局性的调度策略</strong>。Google Borg、Kubernetes使用的就是单体调度设计。</p>]]></content>
      
      
      <categories>
          
          <category> 分布式 </category>
          
          <category> 调度策略 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式 </tag>
            
            <tag> 学习笔记 </tag>
            
            <tag> 调度策略 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式体系结构</title>
      <link href="/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
      <url>/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<p>分布式集群中众多服务器如何进行管理、如何作为统一的资源提供服务呢？这就涉及到分布式体系结构的问题，分布式体系结构分为集中式结构和非集中式结构。</p><h1 id="集中式结构"><a href="#集中式结构" class="headerlink" title="集中式结构"></a>集中式结构</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>集中式结构由一台或多台服务器组成中央服务器，系统内的所有数据都存储在中央服务器中，系统内所有的业务也均先由中央服务器处理。多个节点服务器与中央服务器连接，并将自己的信息汇报给中央服务器，由中央服务器统一进行资源和任务调度：中央服务器根据这些信息，将任务下达给节点服务器；节点服务器执行任务，并将结果反馈给中央服务器。</p><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>结构简单，易于部署</li><li>任务有中央服务器统一管理与调度，无需考虑对任务的多节点部署<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3></li><li>单点瓶颈问题</li><li>单点故障问题</li></ul><h2 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h2><h3 id="Google-Borg"><a href="#Google-Borg" class="headerlink" title="Google Borg"></a>Google Borg</h3><h4 id="名词释义"><a href="#名词释义" class="headerlink" title="名词释义"></a>名词释义</h4><ul><li>Cell：一个集群就是一个Cell</li><li>BorgMaster：中央服务器，每个Cell都有一个Leader就是BorgMaster</li><li>Borglet：其他服务器（节点服务器、从服务器）<h4 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h4><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Borg%E6%9E%B6%E6%9E%84.png" alt="Borg架构"><h4 id="BorgMaster"><a href="#BorgMaster" class="headerlink" title="BorgMaster"></a>BorgMaster</h4>BorgMaster由主进程和独立的 scheduler 进程组成</li><li>主进程：<ul><li>处理客户端RPC请求（任务执行状态更新、查询等）；</li><li>管理系统中所有实体（服务器、任务等）的状态；</li><li>和Borglet通信，周期性轮询Borglet获取节点服务器状态、资源等信息</li></ul></li><li>scheduler 进程<ul><li>调度任务，通过任务对资源的需求和当前Borglet所在服务器的资源情况为任务寻找合适的执行节点<h4 id="Borglet"><a href="#Borglet" class="headerlink" title="Borglet"></a>Borglet</h4>borglet是运行在每个节点机器的agent，负责任务的拉起、停止、重启；管理和搜集本地服务器资源；上报任务状态、服务器状态给BorgMaster<h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><blockquote><ul><li>开发者只需关注应用，不需要关注底层资源管理。它隐藏了资源管理以及错误处理，因此用户能集中精力开发应用。</li><li>高可靠性和可用性，支持多种应用。</li><li>支持上千级服务器的管理和运行。</li></ul></blockquote></li></ul></li></ul><h3 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h3><p>Kubernetes 是用于自动部署、扩展和管理容器化应用程序的开源系统，集群的节点上运行容器化应用，可以进行自动化容器操作，包括部署、调度和在节点间弹性伸缩等。一个Kubernetes集群主要由Master节点和Worker节点组成。<br><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/KubernetesStructure.png" alt="Kubernetes Structure"></p><h4 id="Master节点"><a href="#Master节点" class="headerlink" title="Master节点"></a>Master节点</h4><p>运行在中心服务器，由API Server、Scheduler、Cluster State Store 和 Control Manger Server 组成，负责对集群进行调度管理。</p><ul><li>API Server：是所有 REST 命令的入口，负责处理 REST 的操作并执行相关业务逻辑。</li><li>Scheduler：根据容器需要的资源和当前 Worker 节点服务器的资源信息为容器选择合适的节点服务器。</li><li>Cluster State Store：集群状态存储，默认采用分布式 key-value 存储 etcd，实现共享配置和服务发现。</li><li>Control Manager：执行大部分的集群层次的功能，如执行生命周期功能（命名空间创建和生命周期、事件垃圾收集、已终止垃圾收集、级联删除垃圾收集等）和 API 业务逻辑。<h4 id="Worker节点"><a href="#Worker节点" class="headerlink" title="Worker节点"></a>Worker节点</h4>运行在节点服务器的工作节点，包括Kubelet和kebu-proxy两个核心组件，是负责运行业务应用的容器</li><li>Pod：是 Kubernetes 的最小工作单元，每个 Pod 包含一个或多个容器，Pod 代表着能够作为单一应用程序加以控制的一组容器集合。</li><li>kubelet：通过命令行与 API Server 进行通信，接收 Master 节点根据调度策略发出的请求或命令，在 Worker 节点上管控容器（Pod），并管控容器的运行状态（比如，重新启动出现故障的 Pod）等。</li><li>kube-proxy：负责为容器（Pod）创建网络代理 / 负载平衡服务，从 API Server 获取所有 Server 信息，并根据 Server 信息创建代理服务，这种代理服务称之为 Service。Kube-proxy 主要负责管理 Service 的访问入口，即实现集群内的 Pod 客户端访问 Service，或者是集群外访问 Service，具有相同服务的一组 Pod 可抽象为一个 Service。每个 Service 都有一个虚拟 IP 地址（VIP）和端口号供客户端访问。</li><li>Kube DNS： 负责为整个集群提供 DNS 服务。</li><li>CNI：Container Network Interface 的一个标准的通用接口，用于连接容器管理系统和网络插件。<h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4></li><li>Kubernetes 执行容器编排，自动化容器的部署和复制</li><li>将容器组织为组，弹性伸缩。Kubernetes 通过 Pod 机制 实现了多个容器的协作，能够有效避免将太多功能集中到单一容器镜像这样的错误实践中。此外，软件可以向外扩展跨越多个 Pods 实现初步部署，且相关部署可随时进行规模伸缩。</li><li>容器间负载均衡。Services 用于将具备类似功能的多个 Pod 整合为一组，可轻松进行配置以实现其可发现性、可观察性、横向扩展以及负载均衡。</li><li>易于版本控制与滚动更新。Kubernetes 采取“滚动方式”实现编排，且可跨越部署范围内的全部 Pod。这些滚动更新可进行编排，并以预定义方式配合当前可能尚不可用的 Pods 数量，以及暂时存在的闲置 Pods 数量。Kubernetes 利用新的应用程序镜像版本对已部署 Pods 进行更新，并在发现当前版本存在不稳定问题时回滚至早期部署版本。</li></ul><h3 id="Mesos"><a href="#Mesos" class="headerlink" title="Mesos"></a>Mesos</h3><p>Mesos 也是采用的典型的集中式架构，与 Borg 不同之处在于：</p><ul><li>用户可以向 Borg 的 Master 直接请求任务；</li><li> Mesos 的任务调度框架是双层结构，Mesos 只负责底层资源的管理和分配，并不涉及存储、 任务调度等功能，因此 Mesos Master 对接的是 Spark、Hadoop、Marathon 等框架，用户的任务需要提交到这些框架上。<br>一个集群包括 Mesos Master 和多个 Mesos Agent。Mesos Master 运行在中央服务器，Mesos Agent 运行在节点服务器上。<br><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/MesosStructure.png" alt="Mesos Structure"><h4 id="Mesos-Master"><a href="#Mesos-Master" class="headerlink" title="Mesos Master"></a>Mesos Master</h4>负责收集和管理所有 Agent 所在服务器的资源和状态，并且对接 Spark、Hadoop 等框架，将集群中服务器的资源信息告知给这些框架，以便这些框架进行任务资源匹配和调度。通常采用一主两备的方式，以方便故障处理和恢复。<h4 id="Mesos-Agent"><a href="#Mesos-Agent" class="headerlink" title="Mesos Agent"></a>Mesos Agent</h4>负责任务的拉起、停止、重启等，并负责收集所在服务器的资源 (比如 CPU、内存等) 信息和状态，上报给 Mesos Master。<h4 id="优点-3"><a href="#优点-3" class="headerlink" title="优点"></a>优点</h4></li><li>效率：Mesos 对物理资源进行了逻辑抽象，在应用层而不是物理层分配资源，通过容器而不是虚拟机（VM）分配任务。因为应用程序的调度器知道如何最有效地利用资源，所以在应用层分配资源能够为每个应用程序的特殊需求做考量 ; 而通过容器分配任务则能更好地进行“装箱”。</li><li>可扩展性：Mesos 可扩展设计的关键是两级调度架构，其中 Framework 进行任务调度，Mesos Master 进行资源分配。由于 Master 不必知道每种类型的应用程序背后复杂的调度逻辑，不必为每个任务做调度，因此可以用非常轻量级的代码实现，更易于扩展集群规模。</li><li>模块化：每接入一种新的框架，Master 无需增加新的代码，并且 Agent 模块可以复用，为此开发者可以专注于应用和框架的选择。这，就使得 Mesos 可以支持多种框架，适应不同的应用场景。</li></ul><h1 id="非集中式结构"><a href="#非集中式结构" class="headerlink" title="非集中式结构"></a>非集中式结构</h1><p>非集中式结构中，没有中央服务器和节点服务器之分，所有的服务器地位都是平等的。服务的执行和数据的存储被分散到不同的服务器集群，服务器集群间通过消息传递进行通信和协调。相比于集中式结构，非集中式结构就降低了某一个或者某一簇计算机集群的压力，在解决了单点瓶颈和单点故障问题的同时，还提升了系统的并发度，比较适合大规模集群的管理。</p><h2 id="Gossip-protocol-流言算法-流行病协议"><a href="#Gossip-protocol-流言算法-流行病协议" class="headerlink" title="Gossip protocol - 流言算法/流行病协议"></a>Gossip protocol - 流言算法/流行病协议</h2><blockquote><p>这个协议是一个最终一致性协议，如同其名字一样，工作原理类似于病毒传播。该协议主要用在分布式数据库系统中各个副本节点同步数据。这种场景最大的特点就是网络的节点都是对等节点，是非结构化网络。Gossip 过程是由种子节点发起，当一个种子节点有状态需要更新到网络中的其他节点时，它会随机的选择周围几个节点散播消息，收到消息的节点也会重复该过程，直至最终网络中所有的节点都收到了消息。这个过程可能需要一定的时间，由于不能保证某个时刻所有节点都收到消息，但是理论上最终所有节点都会收到消息。</p></blockquote><h3 id="Gossip优势"><a href="#Gossip优势" class="headerlink" title="Gossip优势"></a>Gossip优势</h3><ul><li>扩展性：允许节点任意增加/减少，新增节点最终状态会与其他节点一致</li><li>容错：节点宕机或者重启不影响Gossip消息传播</li><li>去中心化：所有节点对等，只要网络连通任意一个节点就可以把消息散播全网</li><li>一致性收敛：指数级别传播消息，不一致状态能快速收敛</li><li>过程简单</li></ul><h3 id="Gossip缺陷"><a href="#Gossip缺陷" class="headerlink" title="Gossip缺陷"></a>Gossip缺陷</h3><ul><li>消息延迟：要经过几轮消息传播才能达到最终一致，不适用于实时性要求高的场景</li><li>消息冗余：节点会定期随机选择周围节点发送消息，而收到消息的节点也会重复该步骤，因此就不可避免的存在消息重复发送给同一节点的情况，造成了消息的冗余，增加了收到消息的节点的处理压力</li></ul><blockquote><p>如何优化消息冗余问题？<br>每个节点记录当前传输的消息且还未达到收敛的时候，已经发送给了哪些节点，然后每次选择时从没有发送过的节点列表中随机选择 k 个节点，直到所有节点均被传输或集群收敛为止，减少了重复消息量也加快了收敛速度。</p></blockquote><p>了解了Gossip协议之后，我们通过三种使用Gossip协议的非集中式架构分布式系统来理解非集中式架构。</p><h2 id="Akka集群"><a href="#Akka集群" class="headerlink" title="Akka集群"></a>Akka集群</h2><p><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Akka.png" alt="Akka"></p><h3 id="Actor模型"><a href="#Actor模型" class="headerlink" title="Actor模型"></a>Actor模型</h3><ul><li>Actor： 一个封装了状态和行为的对象，它接收消息并基于该消息执行计算</li><li>MailBox：接受消息，每个 Actor 都有自己的 MailBox<br>Actor 之间互相隔离，不共享内存，但 Actor 之间可通过交换消息（mail）进行通信。比如，在分布式系统中，一个服务器或一个节点可以视为一个 Actor，Actor 与 Actor 之间采用 mail 进行通信。Actor 模型为系统并发度提供了非常好的解决方案，且是一个异步的、非阻塞的、高性能的事件驱动编程模型。</li></ul><p>Akka 框架基于 Actor 模型，提供了一个用于构建可扩展的、弹性的、快速响应的应用程序的平台。Akka 集群是一个完全去中心化的分布式集群管理系统。一个集群由多个节点组成，每个节点都可以进行数据处理和任务执行，节点之间均可进行通信。节点有 Leader 节点和非 Leader 节点之分但Leader 节点只是增加了负责节点的加入和移除集群的功能，并不会影响非集中式结构中节点的平等关系。</p><h3 id="集群组建和管理"><a href="#集群组建和管理" class="headerlink" title="集群组建和管理"></a>集群组建和管理</h3><p>创建集群是节点分为三类</p><ul><li>种子节点：使用静态配置文件方式或者系统运行时指定方式，可以生成种子节点；种子节点是普通节点加入集群的联系点，可以自动接收新加入集群的节点的信息。</li><li>首种子节点：首种子节点是配置文件中的第一个种子节点，其功能是集群第一次启动时，首种子节点启动起来，集群才能组建成功，保证集群第一次创建时只有一个集群。如下图 A 节点，就是 Akka 集群的首种子节点。</li><li>普通节点：可以向种子节点或集群中的任意节点发送 Join 消息，请求加入集群。如下图的 B 和 C 节点，通过向 A 节点发送 Join 消息，从而加入到 Akka 集群。</li></ul><p><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/akka-cluster-gossip.png" alt="集群组建过程"><br>Akka 集群的每个节点启动后，读取配置文件获取种子节点列表，然后开始组建集群：</p><ul><li>如果本节点为首种子节点，则把自己加入到集群列表中，即以自己为中心构建集群；</li><li>如果本节点为种子节点，则向首种子节点请求加入集群，当首种子节点回复同意消息后，可以加入集群，否则不可加入集群；</li><li>如果本节点为普通节点，则可以向任一种子节点（包括首种子节点）请求加入集群，收到同意后，则加入集群，否则不可加入集群。</li></ul><h2 id="Redis集群"><a href="#Redis集群" class="headerlink" title="Redis集群"></a>Redis集群</h2><blockquote><p>什么是Redis？<br>Redis 是一个开源的高性能分布式 key-value 内存数据库，有如下特点：</p><ul><li>支持数据的持久化，可以将内存中的数据保存在磁盘中，重启时可以再次加载并使用；</li><li>支持多种数据结构，不仅支持简单的 key-value 类型的数据，同时还提供 list、set、hash 等数据结构的存储；</li><li>支持数据的备份，即 Master/Slave 模式的数据备份。</li></ul></blockquote><p>Redis 集群是一个分布式（distributed）、容错（fault-tolerant）的 Redis 实现， 集群可以使用的功能是普通单机 Redis 所能使用的功能的一个子集（subset）。Redis集群有如下特点：</p><ul><li>Redis 集群中不存在中央节点，是典型的去中心化结构，每个节点均可与其他节点通信。</li><li>集群中每个节点均存在主备，也就是说每台服务器上都运行两个 Redis 服务，分别为主备，主故障后，备升主。</li><li>所有节点均可负责存储数据、记录集群的状态（包括键值到正确节点的映射），客户端可以访问或连接到任一节点上。</li><li>集群节点同样能自动发现其他节点，检测故障的节点，并在需要的时候在从节点中推选出主节点。<h3 id="Hash槽"><a href="#Hash槽" class="headerlink" title="Hash槽"></a>Hash槽</h3>对于数据的分片存储问题，Redis 集群引入了哈希槽的概念。Redis 集群内置了 16384 个哈希槽，每个节点负责一部分哈希槽。当客户端要存储一个数据或对象时，对该对象的 key 通过 CRC16 校验后对 16384 取模，也就是 HASH_SLOT = CRC16(key) mod 16384 来决定哈希槽，从而确定存储在哪个节点上。<br>Redis 集群采用集群分片方式实现了数据的分片存储，从而将 Redis 的写操作分摊到了多个节点上，提高了写并发能力。</li></ul><h2 id="Cassandra-集群"><a href="#Cassandra-集群" class="headerlink" title="Cassandra 集群"></a>Cassandra 集群</h2><p>Cassandra 的集群架构与数据分片存储方案，与 Redis 集群类似。Cassandra 集群数据存储与 Redis 的不同之处是，Redis 集群每个节点代表一部分哈希槽，一个哈希槽代表一个哈希值区间，而 Cassandra 集群中每个节点代表一个哈希值。在 Cassandra 集群中，每次客户端随机选择集群中的一个节点来请求数据，对应接收请求的节点将对应的 key 在一致性哈希环上定位出是哪些节点应该存储这个数据，然后将请求转发到对应的节点上，并将对应节点的查询反馈返回给客户端。</p>]]></content>
      
      
      <categories>
          
          <category> 分布式 </category>
          
          <category> 体系结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式 </tag>
            
            <tag> 学习笔记 </tag>
            
            <tag> 体系结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式协调与同步——分布式锁</title>
      <link href="/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E4%B8%8E%E5%90%8C%E6%AD%A5%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/"/>
      <url>/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E4%B8%8E%E5%90%8C%E6%AD%A5%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</url>
      
        <content type="html"><![CDATA[<h1 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h1><p><strong>锁是多线程同时访问同一共享资源，保证同一时刻只有一个线程可以访问共享资源所做的一种标记。</strong></p><blockquote><p>在单机多线程环境中，我们经常遇到多个线程访问同一个共享资源（临界资源）的情况。为了维护数据的一致性，我们需要某种机制来保证只有满足某个条件的线程才能访问资源，不满足条件的线程只能等待，在下一轮竞争中重新满足条件时才能访问资源。这个机制指的是在某个地方设立一个<strong>标记</strong>，这个标记每个线程都能看到，其他线程只能等待拥有该标记的线程执行完成，并释放该标记后，才能去申请获取该标记并访问共享资源。这个<strong>“标记”</strong>就是我们说的<strong>锁</strong>。</p></blockquote><p>在大规模分布式系统中，单个机器的线程锁无法管控多个机器对同一资源的访问，这时使用分布式锁，就可以把整个集群当作一个应用一样去处理，实用性和扩展性更好。</p><h1 id="分布式锁"><a href="#分布式锁" class="headerlink" title="分布式锁"></a>分布式锁</h1><p>分布式锁是在分布式环境下，系统部署在多个机器中，实现多进程分布式互斥的一种锁。为了保证多台机器的多个进程都能看到锁，一般锁存在公共储存（如Reids、Memcache、数据库、Zookeeper）。</p><p>目前主流的分布式锁有三种实现方式：</p><ul><li>基于关系型数据库实现，如Mysql</li><li>基于缓存实现，如Redis</li><li>基于ZooKeeper实现分布式锁</li></ul><h2 id="基于关系型数据库实现"><a href="#基于关系型数据库实现" class="headerlink" title="基于关系型数据库实现"></a>基于关系型数据库实现</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>在数据库中创建一张锁表，通过操作该表中数据来实现分布式锁，利用数据库对共享资源的唯一性约束实现分布式锁。当我们要锁住某个资源时，就在表中增加一条记录；想要释放资源就删除这条记录。多个请求同时被提交到数据库时，数据库会保证只有一个操作可以成功，操作成功的那个线程就获得了锁。</p><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>简单，易于理解和实现</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>并发量低，性能差，依赖数据库I/O</li><li>存在单点故障问题，数据库不可用将导致整个系统不可用</li><li>死锁问题，数据库锁没有失效时间，未获得锁的进程只能一直等待已获得锁的进程主动释放锁。一旦已获得锁的进程挂掉或者解锁操作失败，会导致锁记录一直存在数据库中，其他进程无法获得锁。</li></ul><h2 id="基于缓存实现"><a href="#基于缓存实现" class="headerlink" title="基于缓存实现"></a>基于缓存实现</h2><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>基于缓存实现分布式锁是锁相关的数据放在计算机内存中，不需要写入磁盘，从而减少I/O操作。比如基于Redis的setnx(key, value)命令实现分布式锁。key和value是基于缓存的分布式锁的属性，可以表示锁id，value表示当前时间+超时时间，获取分布式锁后如果在value没有释放锁，系统将自动释放锁。</p><blockquote><p>setnx函数返回值</p><ul><li>返回 1，说明该服务器获得锁，setnx 将 key 对应的 value 设置为当前时间 + 锁的有效时间。</li><li>返回 0，说明其他服务器已经获得了锁，进程不能进入临界区。该服务器可以不断尝试 setnx 操作，以获得锁。<br>Redis采取了线程封闭，把任务封闭在一个线程，通过队列来维持进程访问共享资源的先后顺序，当setnx(key, value)函数返回1时表示获得锁，其他进程只能等待获得锁的进程释放锁或者达到超时时间。</li></ul></blockquote><h3 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h3><ul><li>性能好，操作都在内存中进行</li><li>缓存例如Redis可以跨集群部署，避免了单点故障的问题</li><li>很多缓存服务提供了可以实现分布式锁的方法</li><li>可以超时自动放弃锁<h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3></li><li>超时自动释放锁有时候不能够正确执行，或者一个任务执行时间过长，还在访问临界资源，但是锁已经被释放了，这个时候其他进程也可以申请锁并访问临界资源，这显然是不行的。</li></ul><h2 id="基于ZooKeeper实现"><a href="#基于ZooKeeper实现" class="headerlink" title="基于ZooKeeper实现"></a>基于ZooKeeper实现</h2><h3 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h3><blockquote><p>ZooKeeper 基于树形数据存储结构实现分布式锁，来解决多个进程同时访问同一临界资源时，数据的一致性问题。ZooKeeper 的树形数据存储结构主要由 4 种节点构成：</p><ul><li>持久节点。这是默认的节点类型，一直存在于 ZooKeeper 中。</li><li>持久顺序节点。也就是说，在创建节点时，ZooKeeper 根据节点创建的时间顺序对节点进行编号</li><li>临时节点。与持久节点不同，当客户端与 ZooKeeper 断开连接后，该进程创建的临时节点就会被删除。</li><li>临时顺序节点，就是按时间顺序编号的临时节点。</li></ul></blockquote><p>Zookeeper基于临时节点实现分布式锁，假设A、B、C三个进程访问临界资源，分布式锁原理如下：</p><ol><li>Zookeeper中在锁的持久节点目录Lock中为A、B、C三个进程创建了临时顺序节点</li><li>每个进程获取Lock目录下所有临时节点列表，注册子节点变更的Watcher并监听节点</li><li>每个节点确定自己编号是否是Lock下所有子节点中最小的，若最小则获取锁。</li><li>若进程对应的临时节点不是最小的，那么：<ul><li>a. 进程为读请求，如果有比自己序号小的节点中有写请求，则等待</li><li>b. 进程为写请求，如果有比自己序号小的节点中有读请求，则等待</li></ul></li></ol><h3 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h3><ul><li>解决了单点故障、不可重入、死锁等问题</li><li>可靠性最高，有封装好的框架，很容易实现分布式锁的功能<h3 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h3></li><li>性能略弱于基于缓存的实现</li></ul><h2 id="设计分布式锁需要考虑的地方"><a href="#设计分布式锁需要考虑的地方" class="headerlink" title="设计分布式锁需要考虑的地方"></a>设计分布式锁需要考虑的地方</h2><ul><li>互斥性，即在分布式系统环境下，分布式锁应该能保证一个资源或一个方法在同一时间只能被一个机器的一个线程或进程操作。</li><li>具备锁失效机制，防止死锁。即使有一个进程在持有锁的期间因为崩溃而没有主动解锁，也能保证后续其他进程可以获得锁。</li><li>可重入性，即进程未释放锁时，可以多次访问临界资源。</li><li>有高可用的获取锁和释放锁的功能，且性能要好。</li></ul><h1 id="羊群效应"><a href="#羊群效应" class="headerlink" title="羊群效应"></a>羊群效应</h1><blockquote><p>什么是羊群效应<br>在分布式锁问题中，会经常遇到羊群效应。所谓羊群效应，就是在整个分布式锁的竞争过程中，大量的“Watcher 通知”和“子节点列表的获取”操作重复运行，并且大多数节点的运行结果都是判断出自己当前并不是编号最小的节点，继续等待下一次通知，而不是执行业务逻辑。这就会对 ZooKeeper 服务器造成巨大的性能影响和网络冲击。更甚的是，如果同一时间多个节点对应的客户端完成事务或事务中断引起节点消失，ZooKeeper 服务器就会在短时间内向其他客户端发送大量的事件通知。<br>那如何解决这个问题呢？具体方法可以分为以下三步。</p><ol><li>在与该方法对应的持久节点的目录下，为每个进程创建一个临时顺序节点。</li><li>每个进程获取所有临时节点列表，对比自己的编号是否最小，若最小，则获得锁。</li><li>若本进程对应的临时节点编号不是最小的，则继续判断：<ul><li>若本进程为读请求，则向比自己序号小的最后一个写请求节点注册 watch 监听，当监听到该节点释放锁后，则获取锁；</li><li>若本进程为写请求，则向比自己序号小的最后一个读请求节点注册 watch 监听，当监听到该节点释放锁后，获取锁。</li></ul></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> 分布式 </category>
          
          <category> 锁 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式 </tag>
            
            <tag> 学习笔记 </tag>
            
            <tag> 锁 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式协调与同步——分布式事务</title>
      <link href="/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E4%B8%8E%E5%90%8C%E6%AD%A5%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"/>
      <url>/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E4%B8%8E%E5%90%8C%E6%AD%A5%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h1><p>事务是一系列操作组成的工作单元，该工作单元内的操作是不可分割的，即要么所有操作都做，要么所有操作都不做。</p><h2 id="事务的特征-ACID"><a href="#事务的特征-ACID" class="headerlink" title="事务的特征-ACID"></a>事务的特征-ACID</h2><ul><li><strong>原子性（Atomicity）</strong>：事务最终的状态只有全部执行成功或者全部不执行。如果处理事务的任何一项操作不成功，所有操作都会回滚，放佛事务没有被执行过一样。</li><li><strong>一致性（Consistency）</strong>：实务操作前后数据的完整性保持一致，或者满足一致性约束。</li><li><strong>隔离性（Isolation）</strong>：当系统内有多个事务并发执行时，多个事务不会相互干扰，一个事务内部的操作以及使用的数据对其他并发事务是隔离的</li><li><strong>持久性（Durability）</strong>：一个事务完成了那么它对数据库所做的更新就被永久保存下来了</li></ul><h1 id="分布式事务"><a href="#分布式事务" class="headerlink" title="分布式事务"></a>分布式事务</h1><p><strong>分布式事务，就是在分布式系统中运行的事务，由多个本地事务组合而成。在分布式场景下，对事务的处理操作可能来自不同的机器，甚至是来自不同的操作系统。</strong></p><p>分布式事务基本能够满足 ACID，其中的 C 是强一致性，也就是所有操作均执行成功，才提交最终结果，以保证数据一致性或完整性。但随着分布式系统规模不断扩大，复杂度急剧上升，达成强一致性所需时间周期较长，限定了复杂业务的处理。为了适应复杂业务、支持大型分布式系统，出现了 BASE 理论，该理论的一个关键点就是采用<strong>最终一致性代替强一致性</strong>，通过牺牲强一致性，保证最终一致性，来获得高可用性，是对 ACID 原则的弱化。</p><blockquote><p>BASE 理论包括基本可用（Basically Available）、柔性状态（Soft State）和最终一致性（Eventual Consistency）。</p><ul><li>基本可用：分布式系统出现故障的时候，允许损失一部分功能的可用性。比如系统压力过大时会对一些非核心链路的功能进行降级处理。</li><li>柔性状态：在柔性事务中，允许系统存在中间状态，且这个中间状态不会影响系统整体可用性。比如，数据库读写分离，写库同步到读库（主库同步到从库）会有一个延时。</li><li>最终一致性：事务在操作过程中可能会由于同步延迟等问题导致不一致，但最终状态下，数据都是一致的。</li></ul></blockquote><h2 id="分布式事务的实现方法"><a href="#分布式事务的实现方法" class="headerlink" title="分布式事务的实现方法"></a>分布式事务的实现方法</h2><h2 id="基于XA协议的二段提交方法"><a href="#基于XA协议的二段提交方法" class="headerlink" title="基于XA协议的二段提交方法"></a>基于XA协议的二段提交方法</h2><h3 id="XA协议"><a href="#XA协议" class="headerlink" title="XA协议"></a>XA协议</h3><p>XA 是一个分布式事务协议，可以分为两部分，事务管理器和本地资源管理器。<strong>XA</strong>实现分布式事务的原理类似于集中式算法，事务管理器作为协调者负责各个本地资源的提交和回滚，资源管理器就是分布式事务的参与者，通常由数据库实现。</p><h3 id="二段提交协议"><a href="#二段提交协议" class="headerlink" title="二段提交协议"></a>二段提交协议</h3><p>基于XA的二阶段提交方法中的二段提交协议（The two-phase commit protocol，2PC）用于保证分布式系统中事务提交时的数据一致性，是 XA 在全局事务中用于协调多个资源的机制。二段提交协议引入了一个协调者来管理所有的节点，协调者下发请求事务操作，参与者将操作结果通知协调者，协调者根据所有参与者的反馈结果决定各参与者是要提交操作还是撤销操作。二段提交协议分为投票和提交两个阶段。</p><ol><li>第一阶段-投票</li></ol><p>协调者（Coordinator，即事务管理器）会向事务的参与者（Cohort，即本地资源管理器）发起执行操作的 CanCommit 请求，并等待参与者的响应。参与者接收到请求后，会执行请求中的事务操作，记录日志信息但不提交，待参与者执行成功，则向协调者发送“Yes”消息，表示同意操作；若不成功，则发送“No”消息，表示终止操作。当所有的参与者都返回了操作结果（Yes 或 No 消息）后，<strong>系统进入了提交阶段</strong></p><ol start="2"><li>第二阶段-提交</li></ol><p>协调者根据所有参与者返回的信息向参与者发送 DoCommit 或 DoAbort 指令：</p><ul><li>若协调者收到的都是“Yes”消息，则向参与者发送“DoCommit”消息，参与者会完成剩余的操作并释放资源，然后向协调者返回“HaveCommitted”消息；</li><li>如果协调者收到的消息中包含“No”消息，则向所有参与者发送“DoAbort”消息，此时发送“Yes”的参与者则会根据之前执行操作时的回滚日志对操作进行回滚，然后所有参与者会向协调者发送“HaveCommitted”消息；</li><li>协调者接收到“HaveCommitted”消息，整个事务结束。</li></ul><h3 id="不足之处"><a href="#不足之处" class="headerlink" title="不足之处"></a>不足之处</h3><blockquote><ul><li><strong>同步阻塞问题</strong>：二阶段提交算法在执行过程中，所有参与节点都是事务阻塞型的。也就是说，当本地资源管理器占有临界资源时，其他资源管理器如果要访问同一临界资源，会处于阻塞状态。</li><li><strong>单点故障问题：</strong>基于 XA 的二阶段提交算法类似于集中式算法，一旦事务管理器发生故障，整个系统都处于停滞状态。尤其是在提交阶段，一旦事务管理器发生故障，资源管理器会由于等待管理器的消息，而一直锁定事务资源，导致整个系统被阻塞。</li><li><strong>数据不一致问题：</strong>在提交阶段，当协调者向参与者发送 DoCommit 请求之后，如果发生了局部网络异常，或者在发送提交请求的过程中协调者发生了故障，就会导致只有一部分参与者接收到了提交请求并执行提交操作，但其他未接到提交请求的那部分参与者则无法执行事务提交。于是整个分布式系统便出现了数据不一致的问题。</li></ul></blockquote><h2 id="三段提交方法"><a href="#三段提交方法" class="headerlink" title="三段提交方法"></a>三段提交方法</h2><p>三阶段提交协议（Three-phase commit protocol，3PC）引入了超时机制和准备阶段，从而解决二阶段提交的同步阻塞的问题。</p><ul><li>超时机制：协调者或参与者在规定的时间内没有接收到来自其他节点的响应，就会根据当前的状态选择提交或者终止整个事务。</li><li>准备阶段：在第一阶段和第二阶段中间引入准备阶段，在提交阶段之前加入一个预提交阶段，排除不一致的情况，保证最后提交之前各节点状态一致。</li></ul><ol><li>CanCommit阶段</li></ol><p>CanCommit 阶段与 2PC 的投票阶段类似：协调者向参与者发送请求操作（CanCommit 请求），询问参与者是否可以执行事务提交操作，然后等待参与者的响应；参与者收到 CanCommit 请求之后，回复 Yes，表示可以顺利执行事务；否则回复 No。</p><p><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E4%B8%8E%E5%90%8C%E6%AD%A5%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/CanCommit.jpeg" alt="CanCommit"></p><ol start="2"><li>PreCommit阶段</li></ol><p>协调者根据参与者的回复情况，来决定是否可以进行 PreCommit 操作。</p><ul><li>如果所有参与者回复的都是“Yes”，那么协调者就会执行事务的预执行：<ul><li><strong>发送预提交请求。</strong>协调者向参与者发送 PreCommit 请求，进入预提交阶段。</li><li><strong>事务预提交</strong>。参与者接收到 PreCommit 请求后执行事务操作，并将 Undo 和 Redo 信息记录到事务日志中。</li><li><strong>响应反馈</strong>。如果参与者成功执行了事务操作，则返回 ACK 响应，同时开始等待最终指令。</li></ul></li><li>如果任何一个参与者向协调者发送了“No”消息，或者等待超时之后，协调者都没有收到参与者的响应，就执行中断事务的操作：<ul><li><strong>发送中断请求</strong>。协调者向所有参与者发送“Abort”消息。</li><li><strong>终断事务</strong>。参与者收到“Abort”消息之后，或超时后仍未收到协调者的消息，执行事务的终断操作。</li></ul></li></ul><p><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E4%B8%8E%E5%90%8C%E6%AD%A5%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/PreCommit.jpeg" alt="PreCommit"></p><ol start="3"><li>DoCommit阶段</li></ol><p>DoCmmit 阶段进行真正的事务提交，根据 PreCommit 阶段协调者发送的消息，进入执行提交阶段或事务中断阶段。</p><ul><li><strong>执行提交阶段：</strong><ul><li>发送提交请求。协调者接收到所有参与者发送的 Ack 响应，从预提交状态进入到提交状态，并向所有参与者发送 DoCommit 消息。</li><li>事务提交。参与者接收到 DoCommit 消息之后，正式提交事务。完成事务提交之后，释放所有锁住的资源。</li><li>响应反馈。参与者提交完事务之后，向协调者发送 Ack 响应。</li><li>完成事务。协调者接收到所有参与者的 Ack 响应之后，完成事务。</li></ul></li><li><strong>事务中断阶段：</strong><ul><li>发送中断请求。协调者向所有参与者发送 Abort 请求。</li><li>事务回滚。参与者接收到 Abort 消息之后，利用其在 PreCommit 阶段记录的 Undo 信息执行事务的回滚操作，并释放所有锁住的资源。</li><li>反馈结果。参与者完成事务回滚之后，向协调者发送 Ack 消息。</li><li>终断事务。协调者接收到参与者反馈的 Ack 消息之后，执行事务的终断，并结束事务。</li></ul></li></ul><p><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E4%B8%8E%E5%90%8C%E6%AD%A5%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/DoCommit.jpeg" alt="DoCommit"></p><p>参与者向协调者发送 Ack 消息后，如果长时间没有得到协调者的响应，在默认情况下，参与者会自动将超时的事务进行提交，不会像两阶段提交那样被阻塞住。</p><h2 id="基于消息的最终一致性方法"><a href="#基于消息的最终一致性方法" class="headerlink" title="基于消息的最终一致性方法"></a>基于消息的最终一致性方法</h2><p>2PC 和 3PC 这两种方法有一些缺点，比如都需要锁定资源，降低系统性能；都没有真正解决数据不一致的问题。<strong>基于分布式消息的最终一致性方案</strong>引入了一个消息中间件（Message Queue，MQ），用于在多个应用之间进行消息传递。该方法将需要分布式处理的事务通过消息或者日志的方式异步执行，消息或日志可以存到本地文件、数据库或消息队列中，再通过业务规则进行失败重试。</p><p><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E4%B8%8E%E5%90%8C%E6%AD%A5%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/%E5%9F%BA%E4%BA%8EMQ%E7%9A%84%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7%E4%BA%8B%E5%8A%A1.jpeg" alt="基于MQ的最终一致性事务"></p>]]></content>
      
      
      <categories>
          
          <category> 分布式 </category>
          
          <category> 事务 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式 </tag>
            
            <tag> 学习笔记 </tag>
            
            <tag> 事务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式协调与同步——基本概念与算法</title>
      <link href="/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E4%B8%8E%E5%90%8C%E6%AD%A5%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
      <url>/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E4%B8%8E%E5%90%8C%E6%AD%A5%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="分布式互斥"><a href="#分布式互斥" class="headerlink" title="分布式互斥"></a>分布式互斥</h1><ul><li>分布式互斥：分布式系统中排他性的资源访问方式</li><li>临界资源：被互斥访问的共享资源</li></ul><p><strong>分布式互斥可通过如下算法实现互斥访问临界资源。</strong></p><h2 id="集中式算法"><a href="#集中式算法" class="headerlink" title="集中式算法"></a>集中式算法</h2><p>通过一个“协调者”——中央服务器来控制程序对临界资源的访问，示意如下图</p><p><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E4%B8%8E%E5%90%8C%E6%AD%A5%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%AE%97%E6%B3%95/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%92%E6%96%A5-%E9%9B%86%E4%B8%AD%E5%BC%8F%E7%AE%97%E6%B3%95.jpeg" alt="分布式互斥-集中式算法"></p><p>一个程序完成一次临界资源访问，需要如下流程消息交互：</p><ol><li>向协调者发送请求授权信息，1 次消息交互；</li><li>协调者向程序发放授权信息，1 次消息交互；</li><li>程序使用完临界资源后，向协调者发送释放授权，1 次消息交互。<br>集中式算法的优点在于直观、简单、信息交互量少、易于实现，并且所有程序只需和协调者通信，程序之间无需通信。但是协调者处理的消息数量会随着需要访问临界资源的程序数量线性增加，协调者可能会成为系统的性能瓶颈；另一方面，协调者故障，会导致所有的程序均无法访问临界资源，导致整个系统不可用从而引发单点故障问题。因此，在使用集中式算法的时候，一定要选择性能好、可靠性高的服务器来担任协调者。</li></ol><h2 id="分布式算法"><a href="#分布式算法" class="headerlink" title="分布式算法"></a>分布式算法</h2><p>当一个程序需要访问临界资源时，先向系统中的其他程序发送一条请求消息，在接收到所有程序返回的同意消息后，才可访问临界资源。我们把这种算法成为分布式算法，也叫使用组播和逻辑时钟的算法。<br><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E4%B8%8E%E5%90%8C%E6%AD%A5%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%AE%97%E6%B3%95/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%92%E6%96%A5-%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95.jpeg" alt="分布式互斥-分布式算法"><br>一个程序完成一次临界资源的访问，需要进行如下的信息交互：</p><ol><li><p>向其他 n-1 个程序发送访问临界资源的请求，总共需要 n-1 次消息交互；</p></li><li><p>需要接收到其他 n-1 个程序回复的同意消息，方可访问资源，总共需要 n-1 次消息交互。</p></li></ol><p>在大型系统中使用分布式算法，消息数量会随着需要访问临界资源的程序数量呈指数级增加，容易导致高昂的“沟通成本”。分布式算法适合节点数目少且变动不频繁的系统，且由于每个程序均需通信交互，因此适合 P2P 结构的系统。</p><h2 id="令牌环算法"><a href="#令牌环算法" class="headerlink" title="令牌环算法"></a>令牌环算法</h2><p>所有程序构成一个环结构，令牌按照顺时针（或逆时针）方向在程序之间传递，收到令牌的程序有权访问临界资源，访问完成后将令牌传送到下一个程序；若该程序不需要访问临界资源，则直接把令牌传送给下一个程序。<br><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E4%B8%8E%E5%90%8C%E6%AD%A5%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%AE%97%E6%B3%95/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%92%E6%96%A5-%E4%BB%A4%E7%89%8C%E7%8E%AF%E7%AE%97%E6%B3%95.jpeg" alt="分布式互斥-令牌环算法"></p><p>使用临界资源前，令牌环算法不需要像分布式算法那样挨个征求其他程序的意见了，所以相对而言，在令牌环算法里单个程序具有更高的通信效率。同时，在一个周期内，每个程序都能访问到临界资源。因此令牌环算法的公平性很好，非常适合通信模式为令牌环方式的分布式系统。</p><hr><h1 id="分布式选举"><a href="#分布式选举" class="headerlink" title="分布式选举"></a>分布式选举</h1><p><strong>集群</strong><br>集群一般是由两个或两个以上的服务器组建而成，每个服务器都是一个节点。<br>分布式集群中负责协调和管理各节点，让各节点协同工作的节点叫做主节点。主节点可以保证其他节点有序运行，但是如果主节点故障集群就会群龙无首，天下大乱。分布式集群中不可一刻无主，当集群中原先的主节点出现故障不能再提供服务时，就要通过“选举”在集群当前的节点中选出一个新的主节点，保证集群的正常运行。</p><h2 id="分布式选举算法"><a href="#分布式选举算法" class="headerlink" title="分布式选举算法"></a>分布式选举算法</h2><h3 id="Bully算法-ID最大的活跃节点"><a href="#Bully算法-ID最大的活跃节点" class="headerlink" title="Bully算法 - ID最大的活跃节点"></a>Bully算法 - ID最大的活跃节点</h3><p>Bully算法会选取所有活着的节点中 ID 最大的节点作为主节点。<br>在 Bully 算法中，节点的角色有两种：普通节点和主节点。初始化时，所有节点都是平等的，都是普通节点，并且都有成为主的权利。但是，当选主成功后，有且仅有一个节点成为主节点，其他所有节点都是普通节点。当且仅当主节点故障或与其他节点失去联系后，才会重新选主。</p><h4 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h4><ul><li>Election 发起选举</li><li>Active 应答Election</li><li>Victory 竞选成功的主节点向其他节点发送竞选成功的消息</li></ul><h4 id="假设条件"><a href="#假设条件" class="headerlink" title="假设条件"></a>假设条件</h4><ul><li>集群中每个节点都知道其他节点的ID</li></ul><h4 id="具体过程"><a href="#具体过程" class="headerlink" title="具体过程"></a>具体过程</h4><ol><li>集群中每个节点判断自己的 ID 是否为当前活着的节点中 ID 最大的，如果是，则直接向其他节点发送 Victory 消息，宣誓自己的主权；</li><li>如果自己不是当前活着的节点中 ID 最大的，则向比自己 ID 大的所有节点发送 Election 消息，并等待其他节点的回复；</li><li>若在给定的时间范围内，本节点没有收到其他节点回复的 Alive 消息，则认为自己成为主节点，并向其他节点发送 Victory 消息，宣誓自己成为主节点；若接收到来自比自己 ID 大的节点的 Alive 消息，则等待其他节点发送 Victory 消息；</li><li>若本节点收到比自己 ID 小的节点发送的 Election 消息，则回复一个 Alive 消息，告知其他节点，我比你大，重新选举。<br><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E4%B8%8E%E5%90%8C%E6%AD%A5%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%AE%97%E6%B3%95/%E5%88%86%E5%B8%83%E5%BC%8F%E9%80%89%E4%B8%BE-Bully%E7%AE%97%E6%B3%95.jpeg" alt="分布式选举-Bully算法.jpeg"><blockquote><p><strong>实际应用</strong><br>MongoDB副本集故障转移功能，采用节点的最后操作时间表示ID，时间戳最新并且活着的节点是主节点</p></blockquote></li></ol><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul><li>选举速度快</li><li>算法复杂度低</li><li>简单易实现</li></ul><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul><li>需要每个节点有全局的节点信息，因此额外信息存储较多；</li><li>任意一个比当前主节点 ID 大的新节点或节点故障后恢复加入集群的时候，都可能会触发重新选举，成为新的主节点，如果该节点频繁退出、加入集群，就会导致频繁切主。</li></ul><h3 id="Raft算法-民主投票"><a href="#Raft算法-民主投票" class="headerlink" title="Raft算法 - 民主投票"></a>Raft算法 - 民主投票</h3><p>Raft 算法是典型的多数派投票选举算法，核心思想是“少数服从多数”，获得投票最多的节点成为主节点。每一轮选举，每个节点只能投一次票。选主行为是周期进行的，包括选主和任值两个时间段，选主阶段对应投票阶段，任值阶段对应节点成为主之后的任期。但也有例外的时候，如果主节点故障，会立马发起选举，重新选出一个主节点。</p><h4 id="名词解释-1"><a href="#名词解释-1" class="headerlink" title="名词解释"></a>名词解释</h4><ul><li>Leader： 主节点，同一时刻只有一个 Leader，负责协调和管理其他节点；</li><li>Candidate： 候选者，每一个节点都可以成为 Candidate，节点在该角色下才可以被选为新的 -Leader；</li><li>Follower ：Leader 的跟随者，不可以发起选举。</li></ul><h4 id="具体流程"><a href="#具体流程" class="headerlink" title="具体流程"></a>具体流程</h4><ol><li>初始化时，所有节点均为 Follower 状态。</li><li>开始选主时，所有节点的状态由 Follower 转化为 Candidate，并向其他节点发送选举请求。</li><li>其他节点根据接收到的选举请求的先后顺序，回复是否同意成为主。这里需要注意的是，在每一轮选举中，一个节点只能投出一张票。</li><li>若发起选举请求的节点获得超过一半的投票，则成为主节点，其状态转化为 Leader，其他节点的状态则由 Candidate 降为 Follower。Leader 节点与 Follower 节点之间会定期发送心跳包，以检测主节点是否活着。</li><li>当 Leader 节点的任期到了，即发现其他服务器开始下一轮选主周期时，Leader 节点的状态由 Leader 降级为 Follower，进入新一轮选主。<br><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E4%B8%8E%E5%90%8C%E6%AD%A5%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%AE%97%E6%B3%95/%E5%88%86%E5%B8%83%E5%BC%8F%E9%80%89%E4%B8%BE-Raft%E7%AE%97%E6%B3%95.jpeg" alt="分布式选举-Raft算法.jpeg"></li></ol><blockquote><p><strong>实际应用</strong><br>Google 开源的 Kubernetes，擅长容器管理与调度，为了保证可靠性，通常会部署 3 个节点用于数据备份（一主两备）。Kubernetes 的选主采用的是开源的 etcd 组件。etcd 的集群管理器 etcds，是一个高可用、强一致性的服务发现存储仓库，就是采用了 Raft 算法来实现选主和一致性的。</p></blockquote><h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><ul><li>选举速度快</li><li>算法复杂度低</li><li>简单易实现</li><li>选举稳定性比 Bully 算法好，当有新节点加入或节点故障恢复后，会触发选主，但不一定会真正切主，除非新节点或故障后恢复的节点获得投票数过半，才会导致切主。</li></ul><h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><ul><li>它要求系统内每个节点都可以相互通信，且需要获得过半的投票数才能选主成功，通信量大</li></ul><h3 id="ZAB算法-有优先级的民主投票"><a href="#ZAB算法-有优先级的民主投票" class="headerlink" title="ZAB算法 - 有优先级的民主投票"></a>ZAB算法 - 有优先级的民主投票</h3><p>ZAB（ZooKeeper Atomic Broadcast）选举算法是为 ZooKeeper 实现分布式协调功能而设计的。相较于 Raft 算法的投票机制，ZAB 算法增加了通过节点 ID 和数据 ID 作为参考进行选主，节点 ID 和数据 ID 越大，表示数据越新，优先成为主。相比较于 Raft 算法，ZAB 算法尽可能保证数据的最新性。ZAB 算法可以说是对 Raft 算法的改进。</p><h4 id="名词解释-2"><a href="#名词解释-2" class="headerlink" title="名词解释"></a>名词解释</h4><h5 id="3种角色"><a href="#3种角色" class="headerlink" title="3种角色"></a>3种角色</h5><ul><li>Leader： 主节点，同一时刻只有一个 Leader，负责协调和管理其他节点；</li><li>Candidate： 候选者，每一个节点都可以成为 Candidate，节点在该角色下才可以被选为新的 -Leader；</li><li>Follower ：Leader 的跟随者，不可以发起选举。</li></ul><h5 id="4个状态"><a href="#4个状态" class="headerlink" title="4个状态"></a>4个状态</h5><ul><li>Looking 状态，即选举状态。当节点处于该状态时，它会认为当前集群中没有 Leader，因此自己进入选举状态。</li><li>Leading 状态，即领导者状态，表示已经选出主，且当前节点为 Leader。</li><li>Following 状态，即跟随者状态，集群中已经选出主后，其他非主节点状态更新为 Following，表示对 Leader 的追随。</li><li>Observing 状态，即观察者状态，表示当前节点为 Observer，持观望态度，没有投票权和选举权。</li></ul><h4 id="节点三元组信息"><a href="#节点三元组信息" class="headerlink" title="节点三元组信息"></a>节点三元组信息</h4><p><strong>(server_id, server_zxID, epoch) =&gt;（本节点的唯一 ID，本节点存放的数据 ID，当前选取轮数）</strong></p><ul><li>server_id表示本节点的唯一 ID</li><li>server_zxID 表示本节点存放的数据 ID，数据 ID 越大表示数据越新，选举权重越大</li><li>epoch 表示当前选取轮数，一般用逻辑时钟表示</li></ul><h4 id="选票信息"><a href="#选票信息" class="headerlink" title="选票信息"></a>选票信息</h4><p>&lt;epoch, vote_id, vote_zxID&gt; =&gt;（当前选取轮数，被投票节点的ID，被投票节点的服务器zxID）</p><ul><li>epoch 表示当前选取轮数</li><li>vote_id 表示被投票节点的 ID</li><li>vote_zxID 表示被投票节点的服务器 zxID</li></ul><h4 id="选主原则"><a href="#选主原则" class="headerlink" title="选主原则"></a>选主原则</h4><p><strong>server_zxID 最大者成为 Leader；若 server_zxID 相同，则 server_id 最大者成为 Leader</strong></p><h4 id="具体流程-1"><a href="#具体流程-1" class="headerlink" title="具体流程"></a>具体流程</h4><p><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E4%B8%8E%E5%90%8C%E6%AD%A5%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%AE%97%E6%B3%95/%E5%88%86%E5%B8%83%E5%BC%8F%E9%80%89%E4%B8%BE-ZAB%E7%AE%97%E6%B3%95.jpeg" alt="分布式选举-ZAB算法.jpeg"></p><h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4><ul><li>算法性能高</li><li>选举稳定性好，当有新节点加入或节点故障恢复后，会触发选主，但不一定会真正切主，除非新节点或故障后恢复的节点数据 ID 和节点 ID 最大，且获得投票数过半，才会导致切主。</li></ul><h4 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h4><ul><li>采用广播方式发送信息，若节点中有 n 个节点，每个节点同时广播，则集群中信息量为 n*(n-1) 个消息，容易出现广播风暴；</li><li>除了投票，还增加了对比节点 ID 和数据 ID，这就意味着还需要知道所有节点的 ID 和数据 ID，所以选举时间相对较长</li></ul><blockquote><p>为什么“多数派”选主算法通常采用奇数节点，而不是偶数节点呢？<br>多数派选主算法的核心是少数服从多数，获得投票多的节点胜出。如果采用偶数节点集群，当两个节点均获得一半投票时，无法选出主，必须重新投票选举。但即使重新投票选举，两个节点拥有相同投票数的概率也会很大。因此，多数派选主算法通常采用奇数节点。所以 ZooKeeper、 etcd、Kubernetes 等开源软件选主均采用奇数节点的一个关键原因。 </p></blockquote><hr><h1 id="分布式共识"><a href="#分布式共识" class="headerlink" title="分布式共识"></a>分布式共识</h1><p><strong>分布式共识就是在多个节点均可独自操作或记录的情况下，使得所有节点针对某个状态达成一致的过程。</strong>通过共识机制，我们可以使得分布式系统中的多个节点的数据达成一致。从本质上看，分布式选举问题，其实就是传统的分布式共识方法，主要是基于多数投票策略实现的。</p><h2 id="分布式共识方法"><a href="#分布式共识方法" class="headerlink" title="分布式共识方法"></a>分布式共识方法</h2><blockquote><p>区块链中的一个核心概念：挖矿<br>在传统的交易方式中，用户 A 给用户 B 转账，需要银行来实行具体的转账操作并记录交易，银行会从中收取相应的手续费。而采用分布式在线记账的话，参与记录这笔交易的服务器，也可以从中获得一些奖励。所有服务器帮助记录交易并达成一致的过程，就是区块链中的“挖矿”。区块链是由包含交易信息的区块从后向前有序链接起来的数据结构，其中区块是指很多交易数据的集合，每个区块包括区块头和区块体，区块头包括前一区块的哈希值、本区块的哈希值和时间戳；区块体用来存储交易数据。</p></blockquote><h3 id="PoW-算法"><a href="#PoW-算法" class="headerlink" title="PoW 算法"></a>PoW 算法</h3><blockquote><p>以每个节点或服务器的计算能力（即“算力”）来竞争记账权的机制，因此是一种<strong>使用工作量证明机制的共识算法</strong>。也就是说，谁的计算力强、工作能力强，谁获得记账权的可能性就越大。<br>假设每个节点会划分多个区块用于记录用户交易，PoW 算法获取记账权的原理是：利用区块的 index、前一个区块的哈希值、交易的时间戳、区块数据和 nonce 值，通过 SHA256 哈希算法计算出一个哈希值，并判断前 k 个值是否都为 0。如果不是，则递增 nonce 值，重新按照上述方法计算；如果是，则本次计算的哈希值为要解决的题目的正确答案。谁最先计算出正确答案，谁就获得这个区块的记账权。<br>达成共识的过程，就是获得记账权的节点将该区块信息广播给其他节点，其他节点判断该节点找到的区块中的所有交易都是有效且之前未存在过的，则认为该区块有效，并接受该区块，达成一致。<br>PoW 通过“挖矿”的方式发行新币，把比特币分散给个人，实现了相对的公平。PoW 的容错机制，允许全网 50% 的节点出错，因此，如果要破坏系统，则需要投入极大成本（至少要拥有全球 51% 的算力才可尝试攻击比特币）。但是PoW 机制的缺点很明显，共识达成的周期长、效率低，资源消耗大。<br>PoW 机制每次达成共识需要全网共同参与运算，增加了每个节点的计算量，并且如果题目过难，会导致计算时间长、资源消耗多；而如果题目过于简单，会导致大量节点同时获得记账权，冲突多。这些问题，都会增加达成共识的时间。</p><ul><li>nonce 值是用来找到一个满足哈希值的数字</li><li>k 为哈希值前导零的个数，标记了计算的难度，0 越多计算难度越大。</li></ul></blockquote><h3 id="PoS算法"><a href="#PoS算法" class="headerlink" title="PoS算法"></a>PoS算法</h3><blockquote><p>由系统权益代替算力来决定区块记账权，拥有的权益越大获得记账权的概率就越大。这里所谓的权益，就是每个节点占有货币的数量和时间，而货币就是节点所获得的奖励。PoW 算法充分利用了分布式在线记账中的奖励，鼓励“利滚利”。<br>通过 PoS 算法决定区块记账权的流程和 PoW 算法类似，唯一不同的就是，每个节点在计算自己记账权的时候，通过计算自己的股权或权益来评估，如果发现自己权益最大，则将自己的区块广播给其他节点，当然必须保证该区块的有效性。<br>PoS 不需要消耗大量的电力就能够保证区块链网络的安全性，同时也不需要在每个区块中创建新的货币来激励记账者参与当前网络的运行，这也就在一定程度上缩短了达成共识所需要的时间。所以，基于 PoS 算法的以太坊每秒大概能处理 30 笔左右的交易。但PoS 算法中持币越多或持币越久，币龄就会越高，持币人就越容易挖到区块并得到激励，而持币少的人基本没有机会，这样整个系统的安全性实际上会被持币数量较大的一部分人掌握，容易出现垄断现象。</p></blockquote><h3 id="DPoS算法"><a href="#DPoS算法" class="headerlink" title="DPoS算法"></a>DPoS算法</h3><blockquote><p>为了解决 PoS 算法的垄断问题，2014 年比特股（BitShares）的首席开发者丹尼尔 · 拉里默（Dan Larimer）提出了委托权益证明法，也就是 DPoS 算法。</p><p>DPoS 算法的原理，类似股份制公司的董事会制度，普通股民虽然拥有股权，但进不了董事会，他们可以投票选举代表（受托人）代他们做决策。DPoS 是由被社区选举的可信帐户（受托人，比如得票数排行前 101 位）来拥有记账权。</p><p>为了成为正式受托人，用户要去社区拉票，获得足够多的信任。用户根据自己持有的货币数量占总量的百分比来投票。在 DPos 算法中，通常会选出 k(比如 101) 个受托节点，它们的权利是完全相等的。受托节点之间争取记账权也是根据算力进行竞争的。只要受托节点提供的算力不稳定，计算机宕机或者利用手中的权力作恶，随时可以被握着货币的普通节点投票踢出整个系统，而后备的受托节点可以随时顶上去。</p><p>DPoS 是在 PoW 和 PoS 的基础上进行改进的，相比于 PoS 算法，DPoS 引入了受托人，优点主要表现在：</p><ul><li>由投票选举出的若干信誉度更高的受托人记账，解决了所有节点均参与竞争导致消息量大、达成一致的周期长的问题。也就是说，DPoS 能耗更低，具有更快的交易速度。</li><li>每隔一定周期会调整受托人，避免受托人造假和独权。</li></ul><p>但是，在 DPoS 中，由于大多数持币人通过受托人参与投票，投票的积极性并不高；且一旦出现故障节点，DPoS 无法及时做出应对，导致安全隐患。</p></blockquote><h2 id="一致性与共识的区别是什么？"><a href="#一致性与共识的区别是什么？" class="headerlink" title="一致性与共识的区别是什么？"></a>一致性与共识的区别是什么？</h2><blockquote><p><strong>一致性</strong>：分布式系统中的多个节点之间，给定一系列的操作，在约定协议的保障下，对外界呈现的数据或状态是一致的，一致性强调的是结果。<br><strong>共识</strong>：分布式系统中多个节点之间，彼此对某个状态达成一致结果的过程。共识强调的是达成一致的过程，共识算法是保障系统满足不同程度一致性的核心技术。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 分布式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式 </tag>
            
            <tag> 学习笔记 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式综述</title>
      <link href="/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BB%BC%E8%BF%B0/"/>
      <url>/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BB%BC%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="分布式知识体系"><a href="#分布式知识体系" class="headerlink" title="分布式知识体系"></a>分布式知识体系</h1><p>分布式相关技术总的来说可以归纳为如下体系结构：<br><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BB%BC%E8%BF%B0/knowledgeStructure.jpg" alt="四横四纵知识体系"></p><h1 id="分布式是什么？"><a href="#分布式是什么？" class="headerlink" title="分布式是什么？"></a>分布式是什么？</h1><p>分布式是将相同或者相关的程序运行在多台计算机上，从而实现特定目标的一种计算方式。产生分布式的最主要驱动力量，是我们对于性能、可用性及可扩展性的不懈追求。  </p><h2 id="发展过程"><a href="#发展过程" class="headerlink" title="发展过程"></a>发展过程</h2><ul><li>单机模式<br>所有业务和数据均部署到同一台机器上。这种模式的好处是功能、代码和数据集中，便于维护、管理和执行，但计算效率是瓶颈。也就是说单机模式性能受限，也存在单点失效的问题。</li><li>数据并行（数据分布式）<br>对数据进行拆分，利用多台计算机并行执行多个相同任务，通过在相同的时间内完成多个相同任务，从而缩短所有任务的总体执行时间，但对提升单个任务的执行性能及降低时延无效。</li><li>任务并行（任务分布式）<br>单任务拆分成多个子任务，多个子任务并行执行，只要一个复杂任务中的任意子任务的执行时间变短了，那么这个业务的整体执行时间也就变短了。该模式在提高性能、扩展性、可维护性等的同时，也带来了设计上的复杂性问题，比如复杂任务的拆分。  <h2 id="分布式系统的指标"><a href="#分布式系统的指标" class="headerlink" title="分布式系统的指标"></a>分布式系统的指标</h2>分布式的目的是用廉价、普通的机器解决单个计算机处理复杂任务时存在的性能问题、资源瓶颈的问题、可用性和可扩展性问题。用更多机器处理更多的数据和更复杂的任务。所以分布式系统由如下几个重要指标：<h3 id="性能指标（Performance）"><a href="#性能指标（Performance）" class="headerlink" title="性能指标（Performance）"></a>性能指标（Performance）</h3>常见的性能指标，包括吞吐量（Throughput）、响应时间（Response Time）和完成时间（Turnaround Time）</li><li>吞吐量（系统在一定时间内可以处理的任务数）</li></ul><table><thead><tr><th align="center">指标</th><th align="left">释义</th></tr></thead><tbody><tr><td align="center">QPS</td><td align="left">查询数每秒，用于衡量一个系统每秒处理的查询数。这个指标通常用于读操作，越高说明对读操作的支持越好</td></tr><tr><td align="center">TPS</td><td align="left">事务数每秒，用于衡量一个系统每秒处理的事务数。这个指标通常对应写操作，越高说明对写操作的支持越好</td></tr><tr><td align="center">BPS</td><td align="left">比特数每秒，用于衡量一个系统每秒处理的数据量。对于一些网络系统、数据管理系统，请求与请求、事务与事务之间也存在着很大的差异，这种情况下BPS 更能客观地反应系统的吞吐量</td></tr></tbody></table><ul><li>响应时间<br>系统响应一个请求或输入需要花费的时间，响应时间直接影响到用户体验，对于时延敏感的业务非常重要</li><li>完成时间<br>系统真正完成一个请求或处理需要花费的时间。任务并行（也叫作任务分布式）模式出现的其中一个目的，就是缩短整个任务的完成时间。特别是需要计算海量数据或处理大规模任务时，用户对完成时间的感受非常明显。<h3 id="资源占用（Resource-Usage）"><a href="#资源占用（Resource-Usage）" class="headerlink" title="资源占用（Resource Usage）"></a>资源占用（Resource Usage）</h3>资源占用指的是，一个系统提供正常能力需要占用的硬件资源，比如 CPU、内存、硬盘等。</li><li>一个系统在没有任何负载时的资源占用，叫做空载资源占用，体现了这个系统自身的资源占用情况。</li><li>一个系统满额负载时的资源占用，叫做满载资源占用，体现了这个系统全力运行时占用资源的情况，也体现了系统的处理能力。<h3 id="可用性（Availability）"><a href="#可用性（Availability）" class="headerlink" title="可用性（Availability）"></a>可用性（Availability）</h3>系统在面对各种异常时可以正确提供服务的能力。可用性是分布式系统的一项重要指标，衡量了系统的鲁棒性，是系统容错能力的体现。系统的可用性可以用系统停止服务的时间与总的时间之比衡量，还可以用某功能的失败次数与总的请求次数之比来衡量。<blockquote><p><strong>可靠性和可用性区别</strong><br>可靠性通常用来表示一个系统完全不出故障的概率，更多地用在硬件领域。而可用性则更多的是指在允许部分组件失效的情况下，一个系统对外仍能正常提供服务的概率。</p></blockquote><h3 id="可扩展性（Scalability）"><a href="#可扩展性（Scalability）" class="headerlink" title="可扩展性（Scalability）"></a>可扩展性（Scalability）</h3>分布式系统通过扩展集群机器规模提高系统性能 (吞吐、响应时间、 完成时间)、存储容量、计算能力的特性，是分布式系统的特有性质。当任务的需求随着具体业务不断提高时，除了升级系统的性能做垂直（纵向）扩展外，另一个做法就是通过增加机器的方式去水平（横向）扩展系统规模。衡量系统可扩展性的常见指标是<strong>加速比（Speedup）</strong>，也就是一个系统进行扩展后相对扩展前的性能提升。<h2 id="不同场景下分布式系统的指标"><a href="#不同场景下分布式系统的指标" class="headerlink" title="不同场景下分布式系统的指标"></a>不同场景下分布式系统的指标</h2></li></ul><table><thead><tr><th align="center"></th><th align="center">吞吐量</th><th align="center">响应时间</th><th align="center">完成时间</th><th align="center">资源占用</th><th align="center">可用性</th><th align="center">可扩展性</th></tr></thead><tbody><tr><td align="center">电商系统</td><td align="center">极高</td><td align="center">中</td><td align="center">中</td><td align="center">高</td><td align="center">高</td><td align="center">高</td></tr><tr><td align="center">IoT</td><td align="center">低</td><td align="center">较短</td><td align="center">较短</td><td align="center">较低</td><td align="center">中</td><td align="center">中</td></tr><tr><td align="center">电信业务</td><td align="center">高</td><td align="center">短</td><td align="center">短</td><td align="center">低</td><td align="center">极高</td><td align="center">高</td></tr><tr><td align="center">高性能计算机集群</td><td align="center">较高</td><td align="center">中</td><td align="center">极长</td><td align="center">较高</td><td align="center">高</td><td align="center">极高</td></tr><tr><td align="center">大数据</td><td align="center">中</td><td align="center">中</td><td align="center">长</td><td align="center">高</td><td align="center">高</td><td align="center">高</td></tr><tr><td align="center">云计算</td><td align="center">高</td><td align="center">中</td><td align="center">较长</td><td align="center">较高</td><td align="center">高</td><td align="center">高</td></tr><tr><td align="center">区块链</td><td align="center">低</td><td align="center">中</td><td align="center">长</td><td align="center">高</td><td align="center">极高</td><td align="center">低</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 分布式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式 </tag>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac OS更换Shell为zsh</title>
      <link href="/macOS%20Catalina%20Shell/"/>
      <url>/macOS%20Catalina%20Shell/</url>
      
        <content type="html"><![CDATA[<h1 id="Mac-OS更换Shell为zsh"><a href="#Mac-OS更换Shell为zsh" class="headerlink" title="Mac OS更换Shell为zsh"></a>Mac OS更换Shell为zsh</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在2019年WWDC期间，苹果推出了macOS Catalina，并且将zsh设置为操作系统默认shell。我最近因为新换了Macbook 16 2020，才发现默认shell的变化。<br>更改默认 Shell 的操作步骤：<em><strong>系统偏好设置&gt;用户与群组&gt;高级设置&gt;选择shell程序</strong></em></p><h3 id="bash"><a href="#bash" class="headerlink" title="bash"></a>bash</h3><p>shell 俗称壳，是用来与 kernel 内核做区分，作用是给计算机使用者提供操作界面，与计算机内核进行交互。它接收用户命令，对命令做解析，然后调用系统中的应用。<br>shell 有很多种，这里介绍几个常见的shell。<br>第一个 Unix Shell 是1979年底在V7 Unix（AT&amp;T第7版）中引入的，以它的资助者 Stephen Bourne 命名。Bourne shell 是一个交互式命令解释器和命令变成语言。<br>Bourne Again Shell （bash）是GNU计划的一部分，用来替代 Bourne shell。现在大多数Linux发行版都适用bash作为默认的shell。</p><h3 id="zsh"><a href="#zsh" class="headerlink" title="zsh"></a>zsh</h3><p>zsh 号称是「终极shell」，从这个称号看出来它的功能应该很强大</p><h4 id="zsh具有以下主要功能："><a href="#zsh具有以下主要功能：" class="headerlink" title="zsh具有以下主要功能："></a>zsh具有以下主要功能：</h4><ul><li>开箱即用、可编程的命令行补全功能可以帮助用户输入各种参数以及选项</li><li>在用户启动的所有shell中共享命令历史。这一点非常棒，曾经因为sh无法很好的解决多个窗口共享历史命令的问题头疼了一阵儿</li><li>通过扩展的文件通配符，可以不利用外部命令达到find命令一般展开文件名</li><li>改进的变量与数组处理</li><li>在缓冲区中编辑多行命令</li><li>多种兼容模式，例如使用/bin/sh运行时可以伪装成Bourne shell</li><li>可以定制呈现形式的提示符；包括在屏幕右端显示信息，并在键入长命令时自动隐藏</li><li>可加载的模块，提供其他各种支持：完整的TCP与Unix域套接字控制，FTP客户端与扩充过的数学函数</li><li>完全可定制化<br>在使用了一段时间的zsh以后，我发现相比bash确实强大不少，但是其配置确实比较麻烦，好在有一个叫做<a href="https://github.com/ohmyzsh/ohmyzsh">oh-my-zsh</a>的zsh配置，十分强大好用<h2 id="oh-my-zs安装与配置"><a href="#oh-my-zs安装与配置" class="headerlink" title="oh-my-zs安装与配置"></a>oh-my-zs安装与配置</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3></li></ul><ol><li>命令安装</li></ol><table><thead><tr><th align="left">Method</th><th align="left">Command</th></tr></thead><tbody><tr><td align="left"><strong>curl</strong></td><td align="left"><code>sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;</code></td></tr><tr><td align="left"><strong>wget</strong></td><td align="left"><code>sh -c &quot;$(wget -O- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;</code></td></tr><tr><td align="left"><strong>fetch</strong></td><td align="left"><code>sh -c &quot;$(fetch -o - https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;</code></td></tr><tr><td align="left">2. 手动安装</td><td align="left"></td></tr></tbody></table><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh</span><br><span class="line">cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc</span><br><span class="line">source ~/.zshrc</span><br></pre></td></tr></table></figure><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>zsh-autosuggestions 和 zsh-syntax-highlighting 是自定义安装的插件，可以用 git 将插件 clone 到指定插件目录下</p><p>还有一种安装方法是将插件下载下来，将插件文件放入～/.oh-my-zsh/plugins，并且重命名为<strong>插件名.plug.zsh</strong>，最新版的zsh定义了插件名称为&lt;插件名.plug.zsh&gt;，然后在~/.zshrc文件中的plugins属性中加入插件名</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plugins=(</span><br><span class="line">    git </span><br><span class="line">    zsh-autosuggestions</span><br><span class="line">    incr</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="自动提示插件"><a href="#自动提示插件" class="headerlink" title="自动提示插件"></a>自动提示插件</h4><p>  <code>git clone git://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions</code></p><h4 id="语法高亮插件"><a href="#语法高亮插件" class="headerlink" title="语法高亮插件"></a>语法高亮插件</h4><p>  <code>git clone git://github.com/zsh-users/zsh-syntax-highlighting $ZSH_CUSTOM/plugins/zsh-syntax-highlighting</code></p><h4 id="更新配置"><a href="#更新配置" class="headerlink" title="更新配置"></a>更新配置</h4><p>  <code>source ~/.zshrc</code></p><p>需要其他插件的可以自行安装，如果插件未安装，开启终端的时候会报错，按照错误提示，安装对应的插件即可。</p>]]></content>
      
      
      <categories>
          
          <category> 开发工具 </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 开发工具 </tag>
            
            <tag> Shell </tag>
            
            <tag> Mac OS </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
